{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import keyword\n",
    "import heapq\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "import codecs\n",
    "from networkx.algorithms.similarity import graph_edit_distance\n",
    "import networkx as nx\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/12122021/python-implementation-of-a-graph-similarity-grading-algorithm\n",
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_create(edges):\n",
    "    unique=[]\n",
    "    for (i,j) in edges:\n",
    "        if i not in unique:\n",
    "            unique.append(i)\n",
    "        if j not in unique:\n",
    "            unique.append(j)\n",
    "    vertex1={}\n",
    "    for j in range(len(unique)):\n",
    "        vertex2={}\n",
    "        for i in range(len(unique)):\n",
    "            \n",
    "            vertex2[unique[i]]=0\n",
    "        vertex1[unique[j]]=vertex2\n",
    "    for (i,j) in edges:\n",
    "        vertex1[i][j]+=1\n",
    "    adj=[]\n",
    "    for i in vertex1.keys():\n",
    "        temp=[]\n",
    "        for j in vertex1[i].keys():\n",
    "            temp.append(vertex1[i][j])\n",
    "        adj.append(temp)\n",
    "    return np.array(adj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady_sim_neigh(A,B):\n",
    "    G1=nx.from_numpy_matrix(A)\n",
    "    G2=nx.from_numpy_matrix(B)\n",
    "    \n",
    "    w1, v1 = LA.eig(np.matmul(np.transpose(A),np.array(A)))\n",
    "    v1=v1.transpose()\n",
    "    sim1=np.array(np.matmul(np.matrix(A),np.matrix(v1)))\n",
    "    w1, v1 = LA.eig(np.matmul(np.transpose(B),np.array(B)))\n",
    "    v1=v1.transpose()\n",
    "    sim2=np.array(np.matmul(np.matrix(B),np.matrix(v1)))\n",
    "    a1=LA.norm(sim1)\n",
    "    a2=LA.norm(sim2)\n",
    "    a=a1+a2\n",
    "    \n",
    "    w1, v1 = LA.eig(np.matmul(np.array(A),np.transpose(A)))\n",
    "    v1=v1.transpose()\n",
    "    sim1=np.array(np.matmul(np.matrix(A),np.matrix(v1)))\n",
    "    w1, v1 = LA.eig(np.matmul(np.array(B),np.transpose(B)))\n",
    "    v1=v1.transpose()\n",
    "    sim2=np.array(np.matmul(np.matrix(B),np.matrix(v1)))\n",
    "    b1=LA.norm(sim1)\n",
    "    b2=LA.norm(sim2)\n",
    "    b=b1+b2\n",
    "    return (0.5*np.absolute(a1-a2)/float(a)+0.5*np.absolute(b1-b2)/float(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady_sim(A,B):\n",
    "    G1=nx.from_numpy_matrix(A)\n",
    "    G2=nx.from_numpy_matrix(B)\n",
    "    w1, v1 = LA.eig(np.array(A))\n",
    "    v1=v1.transpose()\n",
    "    sim1=np.array(np.matmul(np.matrix(A),np.matrix(v1)))\n",
    "    w1, v1 = LA.eig(np.array(B))\n",
    "    v1=v1.transpose()\n",
    "    sim2=np.array(np.matmul(np.matrix(B),np.matrix(v1)))\n",
    "    a1=LA.norm(sim1)\n",
    "    a2=LA.norm(sim2)\n",
    "    a=a1+a2\n",
    "    return np.absolute(a1-a2)/float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(lap):\n",
    "    sum=np.sum(lap)\n",
    "    lap1=[lap[i]/float(sum) for i in range(len(lap))]\n",
    "    return lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_similarity_measure(a,b):\n",
    "    A=np.matrix(a)\n",
    "    B=np.matrix(b)\n",
    "    G1=nx.from_numpy_matrix(A)\n",
    "    G2=nx.from_numpy_matrix(B)\n",
    "    #Similarity 1\n",
    "    if a.shape==b.shape:\n",
    "        edit_d=nx.graph_edit_distance(G1, G2)\n",
    "    else:\n",
    "        edit_d=0\n",
    "    #Similarity2\n",
    "    iso=nx.is_isomorphic(G1,G2)\n",
    "    d1=np.sum(np.array(A))\n",
    "    d2=np.sum(np.array(B))\n",
    "\n",
    "    \n",
    "    d3=max([d1,d2])\n",
    "    if iso:\n",
    "        return 1\n",
    "        \n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(G1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(G2)\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "    #Similarity 3\n",
    "    lap1=scaling(laplacian1[:k])\n",
    "    lap2=scaling(laplacian2[:k])\n",
    "    eig_similarity = sum((lap1 - lap2)**2)/float(k)\n",
    "    \n",
    "    #Similarity 4\n",
    "    steady_similarity=steady_sim(A,B)\n",
    "    #Similarity 5\n",
    "    steady_similarity1=steady_sim_neigh(A,B)\n",
    "    return (0.25*(1-steady_similarity)+0.25*(1-eig_similarity) +0.25*(edit_d/float(d3))+0.25*(1-steady_similarity1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchCode(a, b):\n",
    "    simscore = 0\n",
    "    l1 = a.splitlines()\n",
    "    l2 = b.splitlines()\n",
    "    i = 0\n",
    "    m = len(l1)\n",
    "    n = len(l2)\n",
    "    while i < min(m,n):\n",
    "        t1=l1[i].split(\" \")\n",
    "        t2=l2[i].split(\" \")\n",
    "        vec1, vec2 = determineWordType(t1, t2)\n",
    "        difvec = np.subtract(vec1, vec2)\n",
    "        if sum(abs(difvec)) < min(len(t1), len(t2)):\n",
    "            simscore += 1\n",
    "        i += 1\n",
    "    if max(m,n)==0:\n",
    "        return 0.0\n",
    "    return simscore/max(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchSlidingWindow(a, b):\n",
    "    simscores = []\n",
    "    l1 = a.splitlines()\n",
    "    l2 = b.splitlines()\n",
    "    m = len(l1)\n",
    "    n = len(l2)\n",
    "    wsize = min(m, n)\n",
    "    i = 0\n",
    "    \n",
    "    if m < n:\n",
    "        i += m\n",
    "        k = 0\n",
    "        while i <= n:\n",
    "            simscores.append(matchCode(''.join(l1), ''.join(l2[k:i])))\n",
    "            k += 1\n",
    "            i += 1      \n",
    "    else:\n",
    "        i += n\n",
    "        k = 0\n",
    "        while i <=m:\n",
    "            simscores.append(matchCode(''.join(l1[k:i]), ''.join(l2)))\n",
    "            k += 1\n",
    "            i += 1\n",
    "    return max(simscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineWordType(t1, t2):\n",
    "    keylist = keyword.kwlist\n",
    "    keys1 = 0\n",
    "    keys2 = 0\n",
    "    cf1 = 0\n",
    "    cf2 = 0\n",
    "    vars1 = 0\n",
    "    vars2 = 0\n",
    "    i = 0\n",
    "    while i < len(t1):\n",
    "        if t1[i] in keylist:\n",
    "            keys1 += 1\n",
    "        elif re.search(r':$', t1[i]):\n",
    "            cf1 += 1\n",
    "        else:\n",
    "            vars1 += 1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    while i < len(t2):\n",
    "        if t2[i] in keylist:\n",
    "            keys2 += 1\n",
    "#         elif re.search(r'[a-zA-Z][a-zA-z0-9]\\(\\):$', t2[i]):\n",
    "#             func2 += 1\n",
    "        elif re.search(r':$', t2[i]):\n",
    "            cf2 += 1\n",
    "        else:\n",
    "            vars2 += 1\n",
    "        i += 1\n",
    "    return [keys1, cf1, vars1], [keys2, cf2, vars2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geeks for geeks \n",
    "def sort_list(list1, list2): \n",
    "  \n",
    "    zipped_pairs = zip(list2, list1) \n",
    "  \n",
    "    z = [x for _, x in sorted(zipped_pairs)] \n",
    "      \n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags(tags,number):\n",
    "    tags.sort()\n",
    "    tags.reverse()\n",
    "#     print(tags)\n",
    "    sum1=0\n",
    "    for i in range(0,number):\n",
    "#         print(tags[i])\n",
    "        sum1+=tags[i]\n",
    "    avg=sum1/float(number)\n",
    "#     print(avg)\n",
    "    i=0\n",
    "    counter=0\n",
    "    while avg<tags[i]:\n",
    "        counter+=1\n",
    "        i+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(array_list):\n",
    "    s=0\n",
    "    for i in range(len(array_list)):\n",
    "        s=s+(array_list[i]**2)\n",
    "    return np.power(s,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_vec(query_vector,vec2):\n",
    "    query_vector=np.array(query_vector)\n",
    "    vec2=np.array(vec2)\n",
    "    q1=magnitude(list(query_vector))\n",
    "    cosine_similarity_val=np.dot(query_vector,vec2)/float(q1*magnitude(list(vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "  m = a.multiply(b)\n",
    "  numerator = m.sum(axis=1)\n",
    "  det_a = np.sqrt(a.sum(axis = 1))\n",
    "  det_b = np.sqrt(b.sum(axis = 1))\n",
    "  return numerator / (det_a * det_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pd):\n",
    "  pd = pd.str.lower()\n",
    "  pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "  pd = pd.apply(lambda x: [item for item in x if item not in removing_words])\n",
    "  pd = pd.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "  pd = pd.str.join(' ')\n",
    "  pd = pd.str.replace('[{}]'.format('$<>?@`\\'\"'), ' ')\n",
    "  return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dash_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if s[i]!='-':\n",
    "            s1=s1+s[i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i]=='0':\n",
    "            s1=s1+'zero'\n",
    "        if s[i]=='1':\n",
    "            s1=s1+'one'\n",
    "        if s[i]=='2':\n",
    "            s1=s1+'two'\n",
    "        if s[i]=='3':\n",
    "            s1=s1+'three'\n",
    "        if s[i]=='4':\n",
    "            s1=s1+'four'\n",
    "        if s[i]=='5':\n",
    "            s1=s1+'five'\n",
    "        if s[i]=='6':\n",
    "            s1=s1+'six'\n",
    "        if s[i]=='7':\n",
    "            s1=s1+'seven'\n",
    "        if s[i]=='8':\n",
    "            s1=s1+'eight'\n",
    "        if s[i]=='9':\n",
    "            s1=s1+'nine'\n",
    "        if s[i]>='0' and s[i]<='9':\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if (s[i]=='.' or s[i]==',' or  s[i]=='!'or  s[i]=='*' or s[i]=='+'  or \n",
    "            s[i]=='-' or s[i]=='\\\"'  or s[i]=='\\'' or\n",
    "            s[i]=='{' or s[i]=='}' or s[i]==';' or s[i]==':' or s[i]=='(' or\n",
    "            s[i]==')' or s[i]=='='  or s[i]=='@' or s[i]=='>' or s[i]=='[' or \n",
    "            s[i]==']' or s[i]=='|' or s[i]=='#' or s[i]=='%' or s[i]=='`' or \n",
    "            s[i]=='~' or s[i]==\"/\" or s[i]=='_' or s[i]=='<' or s[i]=='?' or  \n",
    "            s[i]==' ' or s[i]=='$' or s[i]=='^'or s[i]=='' or s[i]==' ' or s[i]=='&'):\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "        \n",
    "    return s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_query(pd):\n",
    "    temp=[]\n",
    "    temp=tokenizer.tokenize(pd)\n",
    "    temp =[w.lower() for w in temp]\n",
    "    for n in range(len(temp)):\n",
    "        t1=str(dash_improve(str(temp[n])))\n",
    "\n",
    "        temp[n]=t1\n",
    "        t2=str(digit_improve(str(temp[n])))\n",
    "\n",
    "        temp[n]=t2\n",
    "        t3=str(punc_improve(str(temp[n])))\n",
    "\n",
    "        temp[n]=t3\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(m):\n",
    "  b = bs(m)\n",
    "  t = b.find_all('code')\n",
    "  for tag in t:\n",
    "    tag.replace_with('')\n",
    "  k = []\n",
    "  for i in t:\n",
    "    z = str(i).replace(\"<code>\", \"\")\n",
    "    z = str(z).replace(\"</code>\", \"\")\n",
    "    k.append(z)\n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body(m):\n",
    "  b = bs(m)\n",
    "  t = b.find_all('code')\n",
    "  for tag in t:\n",
    "      tag.replace_with('')\n",
    "  a = list({tag.name for tag in b.find_all()})\n",
    "  for i in a:\n",
    "    b = str(b).replace(\"<\"+str(i)+\">\", \" \")\n",
    "    b = str(b).replace(\"</\"+str(i)+\">\", \" \")\n",
    "  return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineWordType(t1, t2):\n",
    "   keylist = keyword.kwlist\n",
    "   keys1 = 0\n",
    "   keys2 = 0\n",
    "#     func1 = 0\n",
    "#     func2 = 0\n",
    "   cf1 = 0\n",
    "   cf2 = 0\n",
    "   vars1 = 0\n",
    "   vars2 = 0\n",
    "   i = 0\n",
    "   while i < len(t1):\n",
    "       if t1[i] in keylist:\n",
    "           keys1 += 1\n",
    "#         elif re.search(r'[a-zA-Z][a-zA-z0-9]*\\(*\\):$', t1[i]):\n",
    "#             func1 += 1\n",
    "       elif re.search(r':$', t1[i]):\n",
    "           cf1 += 1\n",
    "       else:\n",
    "           vars1 += 1\n",
    "       i += 1\n",
    "   i = 0\n",
    "   while i < len(t2):\n",
    "       if t2[i] in keylist:\n",
    "           keys2 += 1\n",
    "#         elif re.search(r'[a-zA-Z][a-zA-z0-9]*\\(*\\):$', t2[i]):\n",
    "#             func2 += 1\n",
    "       elif re.search(r':$', t2[i]):\n",
    "           cf2 += 1\n",
    "       else:\n",
    "           vars2 += 1\n",
    "       i += 1\n",
    "   return [keys1, cf1, vars1], [keys2, cf2, vars2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchCode(a, b):\n",
    "   simscore = 0\n",
    "   l1 = a.splitlines()\n",
    "   l2 = b.splitlines()\n",
    "#     print(l1)\n",
    "#     print(l2)\n",
    "   i = 0\n",
    "   m = len(l1)\n",
    "   n = len(l2)\n",
    "   while i < min(m,n):\n",
    "       t1=l1[i].split(\" \")\n",
    "       t2=l2[i].split(\" \")\n",
    "       vec1, vec2 = determineWordType(t1, t2)\n",
    "       difvec = np.subtract(vec1, vec2)\n",
    "       if sum(abs(difvec)) < min(len(t1), len(t2)):\n",
    "           simscore += 1\n",
    "       i += 1\n",
    "   if max(m,n)==0:\n",
    "     return 0.0\n",
    "   return simscore/max(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceVariables(code):\n",
    "    n_code = []\n",
    "    keylist = keyword.kwlist\n",
    "    lines = code.splitlines()\n",
    "    \n",
    "    for l in lines:\n",
    "        flag=False\n",
    "        arr = l.split(\" \")\n",
    "        \n",
    "        for (i, word) in enumerate(arr):\n",
    "            if re.match(r'\\s*#', word):\n",
    "                flag=True\n",
    "                break\n",
    "            if re.match(r'[a-zA-Z][a-zA-Z0-9]*', word):\n",
    "                if word not in keylist:\n",
    "                    arr[i] = 'var'\n",
    "            else:\n",
    "                arr[i] = word\n",
    "        if flag:\n",
    "            n_code.append(' ')\n",
    "        else:\n",
    "            n_code.append(' '.join(arr))\n",
    "    return '\\n'.join(n_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def print_code(a):\n",
    "    for idx, i in enumerate(a):\n",
    "        print(i, idx)\n",
    "    print(\"\")\n",
    "\n",
    "def get_ws(line):\n",
    "    c = 0\n",
    "    for i in line:\n",
    "        if i in ' ':\n",
    "            c+=1\n",
    "        else:\n",
    "            return c\n",
    "\n",
    "\n",
    "\n",
    "def find_edges_for(states, end):\n",
    "    edges = []\n",
    "    \n",
    "    edges.append((hash(str(states[0])), hash(str(states[1][0]))))\n",
    "    edges.append((hash(str(states[1][-1])), hash(str(states[0]))))\n",
    "    edges.append((hash(str(states[0])), hash(str(end))))\n",
    "\n",
    "    return edges\n",
    "\n",
    "def get_for(lines):\n",
    "#     print_code(lines)\n",
    "\n",
    "    main_stack = []\n",
    "    stack = []\n",
    "    \n",
    "    ws = get_ws(lines[0])\n",
    "\n",
    "    index = 0\n",
    "    edges = []\n",
    "    \n",
    "    main_stack.append(lines[index])\n",
    "\n",
    "    index += 1\n",
    "\n",
    "    while True:\n",
    "        if len(lines) == index:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "            return main_stack,  find_edges_for(main_stack, end) + edges, index\n",
    "        \n",
    "        try:\n",
    "            ws_inside = get_ws(lines[index])\n",
    "        except:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "            return main_stack, find_edges_for(main_stack, end) + edges, index\n",
    "        \n",
    "        if ws_inside == ws:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "#             if get_ws(lines[index]) == ws_inside:\n",
    "            try:\n",
    "                end = lines[index]\n",
    "            except:\n",
    "                pass\n",
    "            return main_stack, find_edges_for(main_stack, end) + edges, index\n",
    "\n",
    "        if ws_inside is None:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "            return main_stack,  find_edges_for(main_stack, end) + edges, index\n",
    "        \n",
    "        if ws_inside > ws:\n",
    "            states, edges_, length = get_general(lines[index:])\n",
    "            edges += edges_\n",
    "            index += length\n",
    "            stack += states\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    return main_stack, [], 0\n",
    "\n",
    "def get_if(lines):\n",
    "#     print_code(lines)\n",
    "    lines.append(\"\\n\")\n",
    "\n",
    "    main_stack = []\n",
    "    \n",
    "    ws = get_ws(lines[0])\n",
    "\n",
    "    if_heads = []\n",
    "    if_s = []\n",
    "    \n",
    "    else_reached = False\n",
    "    \n",
    "    p = []\n",
    "    \n",
    "    c = 0\n",
    "    for i in lines:\n",
    "        ws_i = get_ws(i)\n",
    "        \n",
    "        if ws_i is None:\n",
    "            break\n",
    "        \n",
    "        if ws_i == ws:\n",
    "            if not(i[ws_i:].startswith(\"if\") or i[ws_i:].startswith(\"elif\") or i[ws_i:].startswith(\"else\")):\n",
    "                if_s.append(p)\n",
    "                break\n",
    "            \n",
    "            if_heads.append(i)\n",
    "            if len(p) > 0:\n",
    "                if_s.append(p)\n",
    "                p = []\n",
    "        \n",
    "        if ws_i > ws:\n",
    "            p.append(i)\n",
    "        \n",
    "        c+=1\n",
    "    if len(p) > 0:\n",
    "        if_s.append(p)\n",
    "    \n",
    "    states = []\n",
    "    edges = []\n",
    "    for i, j in zip(if_heads, if_s):\n",
    "        states.append(i)\n",
    "        states_, edges_, _ = get_general(j)\n",
    "        states.append(states_)\n",
    "        edges += edges_\n",
    "    \n",
    "    for idx, i in enumerate(states):\n",
    "        if idx%2 == 0:\n",
    "            try:\n",
    "                edges.append((hash(str(states[idx])), hash(str(states[idx+1]))))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                edges.append((hash(str(states[idx])), hash(str(states[idx+2]))))\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                edges.append((hash(str(states[idx])), hash(str(end))))\n",
    "            except:\n",
    "                pass\n",
    "    return states, edges, c-1\n",
    "\n",
    "# Processing General\n",
    "\n",
    "def find_edges_general(states, end):\n",
    "    edges = []\n",
    "    \n",
    "    edges.append((hash(str(states)), hash(str(states[0]))))\n",
    "    \n",
    "    for idx, i in enumerate(states):\n",
    "        try:\n",
    "            edges.append((hash(str(states[idx])), hash(str(states[idx+1]))))\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        edges.append((hash(str(states[-1])), hash(str(end))))\n",
    "    except:\n",
    "        pass\n",
    "    return edges\n",
    "\n",
    "def get_general(lines):\n",
    "#     print_code(lines)\n",
    "\n",
    "    main_stack = []\n",
    "    stack = []\n",
    "\n",
    "    ws = get_ws(lines[0])\n",
    "\n",
    "    index = 0\n",
    "    edges = []\n",
    "\n",
    "    while True:\n",
    "        if len(lines) == index:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "            return main_stack, find_edges_general(main_stack, end) + edges, index\n",
    "        \n",
    "        ws_inside = get_ws(lines[index])\n",
    "\n",
    "        if ws == ws_inside:\n",
    "            if(lines[index][ws : ws+2] == \"if\"):\n",
    "                states, edges_, length = get_if(lines[index:])\n",
    "                index += length - 1\n",
    "                edges += edges_\n",
    "                main_stack.append(states)\n",
    "            elif lines[index][ws:].startswith(\"for\") or lines[index][ws:].startswith(\"while\"):\n",
    "                if index == len(lines)-1:\n",
    "                    main_stack.append(stack)\n",
    "                    main_stack.append(lines[index])\n",
    "                    end = \"\"\n",
    "                    return main_stack, find_edges_general(main_stack, end) + edges, index\n",
    "                states, edges_, length = get_for(lines[index:])\n",
    "                index += length - 1\n",
    "                edges += edges_\n",
    "                main_stack.append(states)\n",
    "                if index == len(lines):\n",
    "                    end = \"\"\n",
    "                    return main_stack, find_edges_general(main_stack, end) + edges, index\n",
    "            else:\n",
    "                main_stack.append(lines[index])\n",
    "\n",
    "        if ws_inside is None:\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "            end = \"\"\n",
    "            return main_stack, find_edges_general(main_stack, end) + edges, index\n",
    "        \n",
    "        if ws_inside > ws:\n",
    "            states, edges_, length = get_general(lines[index:])\n",
    "            index += length\n",
    "            edges += edges_\n",
    "            main_stack.append(states)\n",
    "            if len(lines) == index:\n",
    "                if len(stack) > 0:\n",
    "                    main_stack.append(stack)\n",
    "                end = \"\"\n",
    "                return main_stack, find_edges_general(main_stack, end) + edges, index\n",
    "\n",
    "        if ws_inside < ws:\n",
    "            end = \"\"\n",
    "            if len(stack) > 0:\n",
    "                main_stack.append(stack)\n",
    "                stack = []\n",
    "                if get_ws(lines[index]) == ws:\n",
    "                    end = lines[index]\n",
    "            return main_stack, find_edges_general(main_stack, end) + edges, index-1\n",
    "        index += 1\n",
    "\n",
    "def get_all_edges(c):\n",
    "    states, edges, length = get_general(c)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e=get_all_edges(code[2][0].split('\\n'))\n",
    "# d=adjacency_create(e)\n",
    "# f=get_all_edges(code[3][0].split('\\n'))\n",
    "# h=adjacency_create(f)\n",
    "# graph_similarity_measure(d,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "needed_words = ['what', 'which', 'if', 'while', 'for', 'between', 'into', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over','then','not','how','do']\n",
    "removing_words = list(set(stop_words).difference(set(needed_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tag_detail = pickle.load(open(\"pfiltered_tags.pickled\",\"rb\"))\n",
    "questions_only= pickle.load(open(\"pfiltered_ques.pickled\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(questions_only)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame(tag_detail)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Tag'].isin([ 'pandas'])].drop_duplicates(subset=['Id'], keep='last').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything into lists\n",
    "id3=processed[0]\n",
    "body=processed[3]\n",
    "title=processed[2]\n",
    "code=processed[4]\n",
    "id3=list(id3)\n",
    "body=list(body)\n",
    "title=list(title)\n",
    "code=list(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_with_hash={}\n",
    "for i in range(len(id3)):\n",
    "    temp={}\n",
    "    temp['body']=body[i]\n",
    "    temp['title']=title[i]\n",
    "    temp['code']=code[i]\n",
    "    questions_with_hash[id3[i]]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1=df1[0]\n",
    "id1=list(id1)\n",
    "tag=df1[1]\n",
    "tag=list(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list=list(set(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_check=[]\n",
    "# counter=0\n",
    "# for i in range(len(tag)):\n",
    "#     counter+=1\n",
    "#     try:\n",
    "#         tag_check.append(preprocess(pd.DataFrame([tag[i]])[0])[0])\n",
    "#     except:\n",
    "#         pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"tag_list_forcheck.txt\", 'wb') as f:\n",
    "#     pickle.dump(tag_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quesid_invertedindex={}\n",
    "for i in range(len(id1)):\n",
    "    if tag[i] not in quesid_invertedindex.keys():\n",
    "        temp=[]\n",
    "        temp.append(id1[i])\n",
    "        quesid_invertedindex[tag[i]]=temp\n",
    "    else:\n",
    "        quesid_invertedindex[tag[i]].append(id1[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag={}\n",
    "for i in quesid_invertedindex.keys():\n",
    "    flag[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list={}\n",
    "counter=0\n",
    "for i in range(len(tag)):\n",
    "    counter+=1\n",
    "    if tag[i]=='python' or tag[i]=='python-2.7' or tag[i]=='python-3.0':\n",
    "        continue\n",
    "    if counter%50000==0:\n",
    "        print(\"Counter : \",counter)\n",
    "    try:\n",
    "        flag2=tag_list[tag[i]]\n",
    "        temp_t=questions_with_hash[id1[i]]     \n",
    "        tag_list[tag[i]]['body']+=temp_t['body']\n",
    "        tag_list[tag[i]]['title']+=temp_t['title']\n",
    "        tag_list[tag[i]]['code'].append(temp_t['code'])\n",
    "        flag[tag[i]]+=1                \n",
    "    except:\n",
    "        temp_t=questions_with_hash[id1[i]]\n",
    "        temp={}\n",
    "        temp['body']=temp_t['body']\n",
    "        temp['title']=temp_t['title']\n",
    "        temp1=[]\n",
    "        temp1.append(temp_t['code'])\n",
    "        temp['code']=temp1\n",
    "        tag_list[tag[i]]=temp\n",
    "        flag[tag[i]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"...........................User Query..........................\")\n",
    "query_body=input(\"Enter the Question Body : \")\n",
    "query_code=input(\"Enter the Question Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning weights to zonal section\n",
    "weight_title=0.3\n",
    "weight_body=0.3\n",
    "weight_code=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(pd.DataFrame([query_body])[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_corpus = pickle.load(open(\"body.txt\", \"rb\") )\n",
    "title_corpus = pickle.load(open(\"title.txt\", \"rb\" ) )\n",
    "tag_list = pickle.load(open(\"tag_list.txt\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #It needed when body.txt and title.txt deleted. \n",
    "\n",
    "# title_corpus=''\n",
    "# body_corpus=''\n",
    "# counter=0\n",
    "# for i in range(len(title)):\n",
    "#     counter+=1\n",
    "#     if counter%20000==0:\n",
    "#         print(\"Counter : \",counter)\n",
    "#     temp=tokenizer.tokenize(title[i])\n",
    "#     temp1=tokenizer.tokenize(body[i])\n",
    "#     for j in range(len(temp)):\n",
    "#         title_corpus+=\" \"+ temp[j]\n",
    "#     for j in range(len(temp1)):\n",
    "#         body_corpus+=\" \"+ temp1[j]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tag_list.txt\", 'wb') as f:\n",
    "    pickle.dump(tag_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"body.txt\", 'wb') as f:\n",
    "    pickle.dump(body_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"title.txt\", 'wb') as f:\n",
    "    pickle.dump(title_corpus, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(query_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_for_query(query_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the questions id with token of query body matched with tags\n",
    "query_body1=preprocess_for_query(query_body)\n",
    "question_id=[]\n",
    "for i in range(len(query_body1)):\n",
    "    if  query_body1[i]==\"python\"  or query_body1[i]=='python-2.7' or query_body1[i]=='python-3.0':\n",
    "        question_id=[]\n",
    "        break\n",
    "    if query_body1[i] in unique_list:\n",
    "        question_id+=quesid_invertedindex[query_body1[i]]\n",
    "    \n",
    "if len(question_id)==0:\n",
    "    question_id+=id3\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus_main=[]\n",
    "title_corpus_main.append(title_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_corpus_main=[]\n",
    "body_corpus_main.append(body_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Title using question body\n",
    "vc_title_tag = CountVectorizer() #TfidfVectorizer()\n",
    "vec_title = vc_title_tag.fit_transform(title_corpus_main)\n",
    "q_title = vc_title_tag.transform(preprocess(pd.DataFrame([query_body])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For body using question body\n",
    "vc_body_tag = CountVectorizer() #TfidfVectorizer()\n",
    "vec_body = vc_body_tag.fit_transform(body_corpus_main)\n",
    "q_title1 = vc_body_tag.transform(preprocess(pd.DataFrame([query_body])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(code):\n",
    "    s=code.split('\\n')\n",
    "    \n",
    "    \n",
    "    t=copy.deepcopy(s)\n",
    "    counter=0\n",
    "    for i in range(len(t)):\n",
    "        if s[counter]==\"\" or s[counter] =='' or s[counter]==' ':\n",
    "            s.pop(counter)\n",
    "        else:\n",
    "            counter+=1\n",
    "    return '\\n'.join(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_code(code1,code2):\n",
    "    if code1=='' or code1==\" \" or code2=='' or code2==\" \" or code1==' ' or code2==' ':\n",
    "        return 0\n",
    "    sim1=matchSlidingWindow(code1, code2)\n",
    "    \n",
    "   \n",
    "    \n",
    "    sim2=graph_similarity_measure(adjacency_create(get_all_edges(pre(code1).split('\\n'))),adjacency_create(get_all_edges(pre(code2).split('\\n'))))\n",
    "    score=0.4*sim1+0.6*sim2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #For body title of tagslist in which I have the body, tiltle and code for each tags at one place.\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# similarity_body=[]\n",
    "# similarity_title=[]\n",
    "# similarity_code=[]\n",
    "# tag_similar=[]\n",
    "# code_similar=[]\n",
    "# counter=0\n",
    "# for i in question_id:\n",
    "#     counter+=1\n",
    "# #     print(\"askf\")\n",
    "#     if counter%2000==0:\n",
    "#         print(\"Counter : \",counter)\n",
    "#     a=[]\n",
    "#     a.append(questions_with_hash[i]['body'])\n",
    "#     sim1=vc_body_tag.transform(a)\n",
    "#     similarity_body.append(pairwise_distances(sim1, q_title1)[0][0])\n",
    "#     a=[]\n",
    "#     a.append(questions_with_hash[i]['title'])\n",
    "#     sim1=vc_title_tag.transform(a)\n",
    "#     similarity_title.append(pairwise_distances(sim1, q_title)[0][0])    \n",
    "#     tag_similar.append(i)\n",
    "#     try:\n",
    "#         sim1=calculate_score_code((' '.join(questions_with_hash[i]['code'])),query_code)\n",
    "#     except:\n",
    "#         print((' '.join(questions_with_hash[i]['code'])))\n",
    "#     similarity_code.append(sim1)\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For body title of tagslist in which I have the body, tiltle and code for each tags at one place.\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# similarity_body=[]\n",
    "# similarity_title=[]\n",
    "# similarity_code=[]\n",
    "# tag_similar=[]\n",
    "# counter=0\n",
    "# for i in tag_list.keys():\n",
    "#     counter+=1\n",
    "#     if counter%2000==0:\n",
    "#         print(\"Counter : \",counter)\n",
    "#     a=[]\n",
    "#     a.append(tag_list[i]['body'])\n",
    "#     sim1=vc_body_tag.transform(a)\n",
    "#     similarity_body.append(pairwise_distances(sim1, q_title1)[0][0])\n",
    "#     a=[]\n",
    "#     a.append(tag_list[i]['title'])\n",
    "#     sim1=vc_title_tag.transform(a)\n",
    "#     similarity_title.append(pairwise_distances(sim1, q_title)[0][0])    \n",
    "#     tag_similar.append(i)\n",
    "# #     sim1=matchCode(tag_list[i]['code'],query_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel=int(input(\"Enter the number of relevant Questions : \"))\n",
    "main_score=[]\n",
    "for i in range(len(similarity_body)):\n",
    "    main_score.append(similarity_body[i]*weight_body+similarity_title[i]*weight_title+similarity_code[i]*weight_code)\n",
    "a=copy.deepcopy(np.array(main_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another approach to take top relevant tags\n",
    "indices_h=heapq.nlargest(rel, range(len(a)), a.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"More Similar Questions for given Query : \",question_id[indices_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning weights to zonal section\n",
    "weight_body1=0.3\n",
    "weight_code1=0.3\n",
    "weight_question1=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers=pickle.load(open(\"pfiltered_ans.pickled\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1   2                                                  3  \\\n",
      "0  497  469   4  open up termin (applications-&gt;utilities-&gt...   \n",
      "1  518  469   2  abl find anyth doe directly. think iter variou...   \n",
      "2  536  502   9  use imagemagick  convert util for this, see ex...   \n",
      "3  538  535  23  one possibl hudson. written in java, there  in...   \n",
      "4  541  535  20  run  a href= http://buildbot.net/trac  buildbo...   \n",
      "\n",
      "                                                   4  \n",
      "0                       [locate InsertFontHere<br/>]  \n",
      "1  [/System/Library/Fonts, /Library/Fonts, ~/Libr...  \n",
      "2  [Convert taxes.pdf taxes.jpg \\n, convert -size...  \n",
      "3                                                 []  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "df2= pd.DataFrame(question_answers)\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything into lists\n",
    "processed=copy.deepcopy(df2)\n",
    "id4=processed[0]\n",
    "body1=processed[3]\n",
    "code1=processed[4]\n",
    "qid=processed[1]\n",
    "score1=processed[2]\n",
    "score1=list(score1)\n",
    "id4=list(id4)\n",
    "body1=list(body1)\n",
    "qid=list(qid)\n",
    "code1=list(code1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_question=question_id[indices_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_corpus1 = pickle.load(open(\"body_list_answers.txt\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  20000\n",
      "Counter :  40000\n",
      "Counter :  60000\n",
      "Counter :  80000\n",
      "Counter :  100000\n",
      "Counter :  120000\n",
      "Counter :  140000\n",
      "Counter :  160000\n",
      "Counter :  180000\n",
      "Counter :  200000\n",
      "Counter :  220000\n",
      "Counter :  240000\n",
      "Counter :  260000\n",
      "Counter :  280000\n",
      "Counter :  300000\n",
      "Counter :  320000\n",
      "Counter :  340000\n",
      "Counter :  360000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-644ab3a4187c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mbody_corpus1\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtemp1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #It needed when body.txt and title.txt deleted. \n",
    "#For answer body\n",
    "body_corpus1=''\n",
    "counter=0\n",
    "for i in range(len(body1)):\n",
    "    counter+=1\n",
    "    if counter%20000==0:\n",
    "        print(\"Counter : \",counter)\n",
    "\n",
    "    temp1=tokenizer.tokenize(body1[i])\n",
    " \n",
    "    for j in range(len(temp1)):\n",
    "        body_corpus1+=\" \"+ temp1[j]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"body_list_answers.txt\", 'wb') as f:\n",
    "    pickle.dump(body_corpus1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_hash={}\n",
    "for i in range(len(id4)):\n",
    "    temp={}\n",
    "    temp['body']=body1[i]\n",
    "\n",
    "    temp['code']=code1[i]\n",
    "    answers_with_hash[id4[i]]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2[1].isin([469])].drop_duplicates(subset=[0], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_corpus_main=[]\n",
    "body_corpus_main.append(body_corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For body using question body\n",
    "vc_body_tag = CountVectorizer() #TfidfVectorizer()\n",
    "vec_body = vc_body_tag.fit_transform(body_corpus_main)\n",
    "q_title1 = vc_body_tag.transform(preprocess(pd.DataFrame([query_body])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_id=[]\n",
    "check_for_q=[]\n",
    "for i in range(len(rel_question)):\n",
    "    pd=df2[df2[1].isin([rel_question[i]])].drop_duplicates(subset=[0], keep='last')\n",
    "    answer_id+=list(pd[0])\n",
    "    check_for_q+=list(pd[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For body title of tagslist in which I have the body, tiltle and code for each tags at one place.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "similarity_body=[]\n",
    "similarity_code=[]\n",
    "tag_similar=[]\n",
    "code_similar=[]\n",
    "counter=0\n",
    "for i in answer_id:\n",
    "    counter+=1\n",
    "#     print(\"askf\")\n",
    "    if counter%2000==0:\n",
    "        print(\"Counter : \",counter)\n",
    "    a=[]\n",
    "    a.append(answers_with_hash[i]['body'])\n",
    "    sim1=vc_body_tag.transform(a)\n",
    "    similarity_body.append(pairwise_distances(sim1, q_title1)[0][0])\n",
    " \n",
    "    tag_similar.append(i)\n",
    "    sim1=calculate_score_code((' '.join(answers_with_hash[i]['code'])),query_code)\n",
    "    similarity_code.append(sim1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel=int(input(\"Enter the number of relevant Answers : \"))\n",
    "main_score=[]\n",
    "for i in range(len(similarity_body)):\n",
    "    value=a[rel_question.index(check_for_q[i])]\n",
    "    main_score.append(similarity_body[i]*weight_body1 + similarity_code[i]*weight_code1+ weight_question1*value)\n",
    "b=copy.deepcopy(np.array(main_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another approach to take top relevant tags\n",
    "indices_h1=heapq.nlargest(rel, range(len(b)), b.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"More Similar Answers for given Query : \",answer_id[indices_h1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answers are : \")\n",
    "rel_ans=answer_id[indices_h1]\n",
    "for i in range(rel):\n",
    "    print(df2[df2[0].isin([rel_ans[i]])].drop_duplicates(subset=[3,4], keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
