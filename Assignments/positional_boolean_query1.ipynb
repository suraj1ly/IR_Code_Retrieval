{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from  nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "import codecs\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def hasNumbers(inputString):\n",
    "    f=False\n",
    "    if len(inputString)==1:\n",
    "        f=True\n",
    "    return (bool(re.search(r'\\d', inputString)) or f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "def relevancy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "#     print(s)\n",
    "    name=[]\n",
    "    name1=''\n",
    "    for i in range(len(s)):\n",
    "        if s[i]>='0' and s[i]<='9':\n",
    "            name=s[i:]\n",
    "            break\n",
    "    for i in range(len(name)):\n",
    "        name1=name1+name[i]\n",
    "    return str(name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if (s[i]=='.' or s[i]==',' or  s[i]=='!'or  s[i]=='*' or s[i]=='+'  or \n",
    "            s[i]=='-' or s[i]=='\\\"'  or s[i]=='\\'' or\n",
    "            s[i]=='{' or s[i]=='}' or s[i]==';' or s[i]==':' or s[i]=='(' or\n",
    "            s[i]==')' or s[i]=='='  or s[i]=='@' or s[i]=='>' or s[i]=='[' or \n",
    "            s[i]==']' or s[i]=='|' or s[i]=='#' or s[i]=='%' or s[i]=='`' or \n",
    "            s[i]=='~' or s[i]==\"/\" or s[i]=='_' or s[i]=='<' or s[i]=='?' or  \n",
    "            s[i]==' ' or s[i]=='$' or s[i]=='^'or s[i]=='' or s[i]==' ' or s[i]=='&'):\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "        \n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dash_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if s[i]!='-':\n",
    "            s1=s1+s[i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i]=='0':\n",
    "            s1=s1+'zero'\n",
    "        if s[i]=='1':\n",
    "            s1=s1+'one'\n",
    "        if s[i]=='2':\n",
    "            s1=s1+'two'\n",
    "        if s[i]=='3':\n",
    "            s1=s1+'three'\n",
    "        if s[i]=='4':\n",
    "            s1=s1+'four'\n",
    "        if s[i]=='5':\n",
    "            s1=s1+'five'\n",
    "        if s[i]=='6':\n",
    "            s1=s1+'six'\n",
    "        if s[i]=='7':\n",
    "            s1=s1+'seven'\n",
    "        if s[i]=='8':\n",
    "            s1=s1+'eight'\n",
    "        if s[i]=='9':\n",
    "            s1=s1+'nine'\n",
    "        if s[i]>='0' and s[i]<='9':\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we wil get the sub-folders\n",
    "os.chdir('20_newsgroups')\n",
    "directories=os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=\"dskgfsur as suraj.pand%ey @gmail.com df,kjfg dfg\\\"dfsfd\\\"dsfk gksd#dsgf874 56438!\"\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# f=tokenizer.tokenize(k)\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminate=['rec.motorcycles','comp.graphics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "files=[]\n",
    "union_set=[]\n",
    "postings={}\n",
    "corpus={}\n",
    "term_frequency={}\n",
    "counter=0\n",
    "for i in range(len(directories)):\n",
    "    files.append(copy.deepcopy([]))\n",
    "\n",
    "for i in range(len(directories)):\n",
    "    print(counter)\n",
    "    counter=counter+1\n",
    "    if directories[i] in eliminate:\n",
    "        \n",
    "        address=os.getcwd()+\"\\\\\"+str(directories[i])\n",
    "        os.chdir(address)\n",
    "        files_in_dir=os.listdir()\n",
    "        for j in range(len(files_in_dir)):\n",
    "\n",
    "            files[i].append(directories[i]+\"/\"+files_in_dir[j])\n",
    "            address1=address+\"\\\\\"+files_in_dir[j]\n",
    "            c=[]\n",
    "            union_set=union_set+[files[i][j]]\n",
    "            offset=0\n",
    "            #stackoverflow\n",
    "            f=codecs.open(address1,'r',encoding='utf_8',errors=\"ignore\")\n",
    "\n",
    "            c = f.readlines()\n",
    "\n",
    "            final=[]\n",
    "            in1=c.index('\\n')\n",
    "            for k in range((in1+1),len(c)):\n",
    "\n",
    "                final.append(c[k])\n",
    "            #Final is a list of sentences \n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            #Tokenizer previously used\n",
    "    #         tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "            temp1=[]\n",
    "            for k in range(len(final)):\n",
    "                temp=[]\n",
    "                temp=tokenizer.tokenize(final[k])\n",
    "                temp =[w.lower() for w in temp]\n",
    "                temp=[lemmatizer.lemmatize(w,pos='v') for w in temp]\n",
    "             \n",
    "    #             print(\"Stop Wro\",temp)\n",
    "\n",
    "                for n in range(len(temp)):\n",
    "#                     if hasNumbers(temp[n]):\n",
    "#                         continue\n",
    "#                     if temp[n].isdigit():\n",
    "#                         continue\n",
    "                    t1=str(dash_improve(str(temp[n])))\n",
    "\n",
    "                    temp[n]=t1\n",
    "                    t2=str(digit_improve(str(temp[n])))\n",
    "\n",
    "                    temp[n]=t2\n",
    "                    t3=str(punc_improve(str(temp[n])))\n",
    "\n",
    "                    temp[n]=t3\n",
    "\n",
    "                    if temp==list(['']) or temp==list([' ']) or temp[n]=='' or temp[n]==' ':\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp1=temp1+temp\n",
    "\n",
    "            corpus[files[i][j]]=temp1\n",
    "    #         if (counter % 5000)==0:\n",
    "    #             print(\"Count \",counter)\n",
    "    #         counter=counter+1\n",
    "            for k in set(corpus[files[i][j]]):\n",
    "\n",
    "                if k in postings.keys():\n",
    "                    if files[i][j] in postings[k]:\n",
    "                        pass\n",
    "                    else:\n",
    "\n",
    "                        postings[k].append(files[i][j])\n",
    "                    term_frequency[k]=term_frequency[k]+1\n",
    "                else:\n",
    "                    s=[]\n",
    "                    s.append(files[i][j])\n",
    "                    postings[k]=s\n",
    "                    term_frequency[k]=1  \n",
    "\n",
    "        os.chdir('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'article',\n",
       " '1993apr3',\n",
       " '183303',\n",
       " '6442',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " '1993apr3',\n",
       " '183303',\n",
       " '6442',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " '183303',\n",
       " '6442',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " '6442',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jna8182',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'in',\n",
       " 'article',\n",
       " 'onenineninethreeaprthree',\n",
       " 'oneeightthreethreezerothree',\n",
       " 'sixfourfourtwo',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'jnaeightoneeighttwo',\n",
       " 'ucs',\n",
       " 'usl',\n",
       " 'edu',\n",
       " 'armstrong',\n",
       " 'jay',\n",
       " 'n',\n",
       " 'write',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'can',\n",
       " 'someone',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'can',\n",
       " 'ftp',\n",
       " 'dta',\n",
       " 'or',\n",
       " 'dmorph',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'dmorf',\n",
       " 'dave',\n",
       " 's',\n",
       " 'morph',\n",
       " 'i',\n",
       " 'think',\n",
       " 'be',\n",
       " 'what',\n",
       " 'it',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'dave',\n",
       " 's',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdos_uploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdosuploads',\n",
       " 'directory',\n",
       " 'tga',\n",
       " 'assembler',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'msdosuploads',\n",
       " 'directory',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wuarchive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wuarchive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wuarchive',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'they',\n",
       " 'be',\n",
       " 'arjed',\n",
       " 'and',\n",
       " 'bundle',\n",
       " 'with',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'xmemory',\n",
       " 'versions',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'dmorfx',\n",
       " 'exe',\n",
       " 'and',\n",
       " 'dtax',\n",
       " 'exe',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'a',\n",
       " 'version',\n",
       " 'of',\n",
       " 'aaplay',\n",
       " 'exe',\n",
       " 'there',\n",
       " 'with',\n",
       " 'which',\n",
       " 'you',\n",
       " 'can',\n",
       " 'view',\n",
       " 'file',\n",
       " 'you',\n",
       " 'create',\n",
       " 'with',\n",
       " 'dta',\n",
       " 'exe',\n",
       " 'or',\n",
       " 'there',\n",
       " 'with',\n",
       " 'which',\n",
       " 'you',\n",
       " 'can',\n",
       " 'view',\n",
       " 'file',\n",
       " 'you',\n",
       " 'create',\n",
       " 'with',\n",
       " 'dta',\n",
       " 'exe',\n",
       " 'or',\n",
       " 'there',\n",
       " 'with',\n",
       " 'which',\n",
       " 'you',\n",
       " 'can',\n",
       " 'view',\n",
       " 'file',\n",
       " 'you',\n",
       " 'create',\n",
       " 'with',\n",
       " 'dta',\n",
       " 'exe',\n",
       " 'or',\n",
       " 'there',\n",
       " 'with',\n",
       " 'which',\n",
       " 'you',\n",
       " 'can',\n",
       " 'view',\n",
       " 'file',\n",
       " 'you',\n",
       " 'create',\n",
       " 'with',\n",
       " 'dta',\n",
       " 'exe',\n",
       " 'or',\n",
       " 'there',\n",
       " 'with',\n",
       " 'which',\n",
       " 'you',\n",
       " 'can',\n",
       " 'view',\n",
       " 'file',\n",
       " 'you',\n",
       " 'create',\n",
       " 'with',\n",
       " 'dta',\n",
       " 'exe',\n",
       " 'or',\n",
       " 'there',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['comp.graphics/37914']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count  5000\n",
      "Count  10000\n",
      "Count  15000\n",
      "Count  20000\n",
      "Count  25000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter=0\n",
    "positional_indexing={}\n",
    "term_frequency={}\n",
    "for i in postings.keys():\n",
    "#     print(\"hello\")\n",
    "    \n",
    "    counter=counter+1\n",
    "    if (counter % 5000)==0:\n",
    "        print(\"Count \",counter)\n",
    "    document_detail={}\n",
    "    for j in postings[i]:\n",
    "#         for k in corpus[j]:\n",
    "        indices=[]\n",
    "    #Stackoverflow\n",
    "        doc=[]\n",
    "        doc=list(corpus[j])\n",
    "        indices = [m for m, x in enumerate(doc) if x == i]\n",
    "        document_detail[j]=indices\n",
    "    positional_indexing[i]=document_detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the phrase query : image processing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6942c9b6ea61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the phrase query : \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrelevant_doc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\s+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Phrase Query \n",
    "query=input(\"Enter the phrase query : \")\n",
    "relevant_doc=[]\n",
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "temp=copy.deepcopy(query)\n",
    "temp=tokenizer.tokenize(temp)\n",
    "\n",
    "# print(temp)\n",
    "tem=[]\n",
    "start=[]\n",
    "tem=copy.deepcopy(temp)\n",
    "tem = copy.deepcopy([w.lower() for w in tem])\n",
    "tem=copy.deepcopy([lemmatizer.lemmatize(w,pos='v') for w in tem])\n",
    "#     tem=copy.deepcopy(list(set(tem)-set(stop_words)))\n",
    "temp1=[]\n",
    "# print(\"Second\",tem)\n",
    "for n in range(len(tem)):\n",
    "\n",
    "    t1=str(dash_improve(str(tem[n])))\n",
    "\n",
    "    tem[n]=t1\n",
    "    t2=str(digit_improve(str(tem[n])))\n",
    "\n",
    "    tem[n]=t2\n",
    "    t3=str(punc_improve(str(tem[n])))\n",
    "\n",
    "    tem[n]=t3\n",
    "    if tem==list(['']) or tem==list([' ']) or tem[n]=='' or tem[n]==' ':\n",
    "        pass\n",
    "    else:\n",
    "        s=[]\n",
    "        s.append(tem[n])\n",
    "        temp1=temp1+s\n",
    "# print(\"Main\",temp1)\n",
    "# print(temp1)\n",
    "# print(np.array(f).shape)\n",
    "# for i in range(len(temp2)):\n",
    "#     for j in range(len(temp2[i])):\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions=[]\n",
    "doc_found=[]\n",
    "for i in range(len(temp1)):\n",
    "\n",
    "    d=positional_indexing[temp1[i]]\n",
    "#     print(d)\n",
    "    h=[]\n",
    "    h=h+list(d.keys())\n",
    "    doc_found.append(h)\n",
    "#     positions.append(d[doc_found[len(doc_found)-1]])\n",
    "# print(doc_found[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=doc_found[0]\n",
    "for i in range(1,len(doc_found)):\n",
    "    start=list(set(start).intersection(set(doc_found[i])))\n",
    "# print(\"Start :\",start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'process']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp2=[]\n",
    "f=[]\n",
    "for i in range(len(temp1)):\n",
    "    d=positional_indexing[temp1[i]]\n",
    "#     print(d)\n",
    "    temp2=[]\n",
    "    l=[]\n",
    "    for j in d.keys():\n",
    "    \n",
    "        if j in start:\n",
    "#             print(j)\n",
    "            h=[]\n",
    "            h=h+list(d[j])\n",
    "            l.append(h)\n",
    "    f.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents :  ['comp.graphics/38799', 'comp.graphics/38400', 'comp.graphics/38417', 'comp.graphics/38292', 'comp.graphics/38692', 'comp.graphics/38898', 'comp.graphics/38824', 'comp.graphics/39078', 'comp.graphics/38726', 'comp.graphics/39638', 'comp.graphics/39080', 'comp.graphics/38980', 'comp.graphics/38403', 'comp.graphics/38297', 'comp.graphics/38851', 'comp.graphics/39047', 'comp.graphics/38377', 'comp.graphics/38462', 'comp.graphics/38944', 'comp.graphics/39052', 'comp.graphics/38861', 'comp.graphics/39039', 'comp.graphics/39003', 'comp.graphics/38682', 'comp.graphics/38456', 'comp.graphics/38233', 'comp.graphics/38778', 'comp.graphics/38452', 'comp.graphics/38375', 'comp.graphics/38257', 'comp.graphics/39081', 'comp.graphics/39651', 'comp.graphics/38762', 'comp.graphics/38241', 'comp.graphics/38877', 'comp.graphics/37945', 'comp.graphics/38454', 'comp.graphics/38215', 'comp.graphics/39642', 'comp.graphics/38870', 'comp.graphics/38976', 'comp.graphics/38945', 'comp.graphics/38852', 'comp.graphics/38821', 'comp.graphics/38871', 'comp.graphics/38784', 'comp.graphics/39632', 'comp.graphics/38299', 'comp.graphics/38963', 'comp.graphics/38622', 'comp.graphics/39007', 'comp.graphics/38731', 'comp.graphics/38853', 'comp.graphics/38416', 'comp.graphics/38409', 'comp.graphics/38993', 'comp.graphics/38897', 'comp.graphics/38926', 'comp.graphics/38577', 'comp.graphics/38460', 'comp.graphics/38244', 'comp.graphics/38376']\n"
     ]
    }
   ],
   "source": [
    "relevant_doc=[]\n",
    "if len(f)==1:\n",
    "    relevant_doc=copy.deepcopy(start)\n",
    "else:\n",
    "#     initial=copy.deepcopy(f[0])\n",
    "    for i in range(len(f[0])):\n",
    "        \n",
    "        for j in range(0,(len(f)-1)):\n",
    "            for k in range(len(f[j][i])):\n",
    "                count=0\n",
    "                for count in range(0,(len(f)-1)):\n",
    "                    if (f[j][i][k]+count) in f[j+count][i]:\n",
    "                        count=count+1\n",
    "                    else:\n",
    "                        break\n",
    "                if count==(len(f)-1):\n",
    "                    relevant_doc.append(start[i])\n",
    "                    break\n",
    "\n",
    "#             flag=relevancy(f[j][i],f[j+1][i])\n",
    "#             if flag==1:\n",
    "#                 count=count+1\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 break\n",
    "#         if count==(len(f)-1):\n",
    "#             relevant_doc.append(start[i])\n",
    "print(\"Relevant documents : \",relevant_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=time.time()\n",
    "print(\"Running Time for the quesry  : \",(end-start),\"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
