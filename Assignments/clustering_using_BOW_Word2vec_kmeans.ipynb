{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from  nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "import codecs\n",
    "import collections \n",
    "import seaborn\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_word(cluster):\n",
    "    pur=0\n",
    "    n=len(cluster)\n",
    "    sum=0\n",
    "    for i in range(len(cluster)):\n",
    "        count=[0]*n\n",
    "        sum+=len(cluster[i])\n",
    "        for j in range(len(cluster[i])):\n",
    "            count[cluster[i][j]]+=1\n",
    "        pur+=np.max(count)\n",
    "    return pur/float(sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(cluster):\n",
    "    pur=0\n",
    "    n=len(cluster)\n",
    "    sum=0\n",
    "    for i in range(len(cluster)):\n",
    "        count=[0]*n\n",
    "        sum+=len(cluster[i])\n",
    "        for j in range(len(cluster[i])):\n",
    "            count[cluster[i][j]-1]+=1\n",
    "        pur+=np.max(count)\n",
    "    return pur/float(sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(rep1,rep2):\n",
    "    return np.array_equal(rep1,rep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(data1,point):\n",
    "    sum=0\n",
    "    for i in range(len(data1)):\n",
    "        sum+=(data1[i]-point[i])**2\n",
    "    return sum**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_distance(test,points):\n",
    "    euc=[]\n",
    "    for i in range(len(points)):\n",
    "        euc.append(euclidean(points[i],test))\n",
    "    return np.argmin(np.array(euc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representatitive(clusters):\n",
    "    rep=[]\n",
    "    end=(len(clusters[0][0])-1)\n",
    "    for i in range(len(clusters)):\n",
    "        rep.append(np.mean(clusters[i][:end],axis=0).tolist())\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(corpus):\n",
    "    sum=0\n",
    "    for i in corpus.keys():\n",
    "        sum+=corpus[i]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(test,points):\n",
    "    euc=[]\n",
    "    end=(len(points[0])-1)\n",
    "    for i in range(len(points)):\n",
    "        euc.append(euclidean(points[i][:end],test[:end]))\n",
    "    return np.argmin(np.array(euc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(tf_idf,num,vocub):\n",
    "    # To threshold the features\n",
    "    threshold=num/float(vocub)\n",
    "    sort_by_value = sorted(tf_idf.items(),key=lambda value: value[1],reverse=True)\n",
    "    vocub_main=[]\n",
    "    for i in range(int(len(sort_by_value)*threshold)):\n",
    "        vocub_main.append(sort_by_value[i][0])\n",
    "    return vocub_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss_calculate(rep,clusters):\n",
    "    rss=0\n",
    "    end=(len(rep[0])-1)\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            rss+=euclidean(clusters[i][j][:end],rep[i][:end])\n",
    "    return rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_word2vec(data1,k,num):\n",
    "    clusters=[]\n",
    "    for i in range(k):\n",
    "        clusters.append([])\n",
    "    store=copy.deepcopy(clusters)\n",
    "#     num=[i for i in range(len(data1))]\n",
    "#     random.shuffle(num)\n",
    "    points=num[0:k]\n",
    "    rep=[data1[points[i]] for i in range(len(points))]\n",
    "    counter=0\n",
    "    flag=True\n",
    "    end=(len(rep[0])-1)\n",
    "    rep_initial=copy.deepcopy(rep)\n",
    "    while counter<100 or flag:\n",
    "        if flag==False:\n",
    "            if similar(rep_initial[:end],rep[:end]):\n",
    "                break\n",
    "        flag=False\n",
    "        rep_initial=copy.deepcopy(rep[:end])\n",
    "        print(\"Counter : \",counter)\n",
    "        for i in range(len(data1)):\n",
    "            l=find_index(data1[i],rep)\n",
    "            clusters[l].append(data1[i])\n",
    "        rep=representatitive(clusters)\n",
    "        result=copy.deepcopy(clusters)\n",
    "        clusters=copy.deepcopy(store)\n",
    "        counter+=1\n",
    "    rss_cal=rss_calculate(rep,result)\n",
    "    return result,rss_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data1,k):\n",
    "    clusters=[]\n",
    "    for i in range(k):\n",
    "        clusters.append([])\n",
    "    store=copy.deepcopy(clusters)\n",
    "    num=[i for i in range(len(data1))]\n",
    "    random.shuffle(num)\n",
    "    points=num[0:k]\n",
    "    rep=[data1[points[i]] for i in range(len(points))]\n",
    "    counter=0\n",
    "    flag=True\n",
    "    end=len(rep[0])-1\n",
    "    rep_initial=copy.deepcopy(rep)\n",
    "    while counter<100 or flag:\n",
    "        if flag==False:\n",
    "            if similar(rep_initial[:end],rep[:end]):\n",
    "                break\n",
    "        flag=False\n",
    "        rep_initial=copy.deepcopy(rep[:end])\n",
    "        print(\"Counter : \",counter)\n",
    "        for i in range(len(data1)):\n",
    "            l=find_index(data1[i],rep)\n",
    "            clusters[l].append(data1[i])\n",
    "        rep=representatitive(clusters)\n",
    "        result=copy.deepcopy(clusters)\n",
    "        clusters=copy.deepcopy(store)\n",
    "        counter+=1\n",
    "    rss_cal=rss_calculate(rep,result)\n",
    "    return result,rss_cal,num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size(dict_list):\n",
    "    sum=0\n",
    "    for i in dict_list.keys():\n",
    "        sum+=dict_list[i]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict,true):\n",
    "    count=0\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i]==true[i]:\n",
    "            count+=1\n",
    "    return count/float(len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geeks for geeks \n",
    "def sort_list(list1, list2): \n",
    "    zipped_pairs = zip(list2, list1) \n",
    "    z = [x for _, x in sorted(zipped_pairs)] \n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "#     print(s)\n",
    "    name=[]\n",
    "    name1=''\n",
    "    for i in range(len(s)):\n",
    "        if s[i]>='0' and s[i]<='9':\n",
    "            name=s[i:]\n",
    "            break\n",
    "    for i in range(len(name)):\n",
    "        name1=name1+name[i]\n",
    "    return str(name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if (s[i]=='.' or s[i]==',' or  s[i]=='!'or  s[i]=='*' or s[i]=='+'  or s[i]=='-' or s[i]=='\\\"'  or s[i]=='\\'' or\n",
    "            s[i]=='{' or s[i]=='}' or s[i]==';' or s[i]==':' or s[i]=='(' or s[i]==')' or\n",
    "        s[i]=='='  or s[i]=='@' or s[i]=='>' or s[i]=='[' or s[i]==']' or s[i]=='|' \n",
    "        or s[i]=='#' or s[i]=='%' or s[i]=='`' or s[i]=='~' or s[i]==\"/\" or s[i]=='_' or s[i]=='<' or s[i]=='?' or  s[i]==' ' or s[i]=='$' or s[i]=='^'or s[i]=='' or s[i]==' ' or s[i]=='&'):\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "        \n",
    "    return s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dash_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if s[i]!='-':\n",
    "            s1=s1+s[i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_improve(str1):\n",
    "    s=[]\n",
    "    s1=''\n",
    "    for i in range(len(str1)):\n",
    "        s.append(str1[i])\n",
    "#     print(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i]=='0':\n",
    "            s1=s1+'zero'\n",
    "        if s[i]=='1':\n",
    "            s1=s1+'one'\n",
    "        if s[i]=='2':\n",
    "            s1=s1+'two'\n",
    "        if s[i]=='3':\n",
    "            s1=s1+'three'\n",
    "        if s[i]=='4':\n",
    "            s1=s1+'four'\n",
    "        if s[i]=='5':\n",
    "            s1=s1+'five'\n",
    "        if s[i]=='6':\n",
    "            s1=s1+'six'\n",
    "        if s[i]=='7':\n",
    "            s1=s1+'seven'\n",
    "        if s[i]=='8':\n",
    "            s1=s1+'eight'\n",
    "        if s[i]=='9':\n",
    "            s1=s1+'nine'\n",
    "        if s[i]>='0' and s[i]<='9':\n",
    "            pass\n",
    "        else:\n",
    "            s1=s1+s[i]\n",
    "    return s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp={}\n",
    "# for j in train:\n",
    "\n",
    "#     temp[j]=class_wise[i][j]\n",
    "# class_wise1.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(class_wise,train_size,test_size):\n",
    "    train_size=int(train_size*len(class_wise[0])/float(100))\n",
    "    test_size=len(class_wise[0].keys())-train_size\n",
    "    train_set={}\n",
    "    test_set={}\n",
    "    class_wise1=[]\n",
    "    class_wise2=[]\n",
    "    for i in range(len(class_wise)):\n",
    "        keys=class_wise[i].keys()\n",
    "        shuffled_data=random.sample(keys, len(keys))\n",
    "        train=shuffled_data[:train_size]\n",
    "        test=shuffled_data[train_size:]\n",
    "        train_data=[]\n",
    "        test_data=[]\n",
    "        temp={}\n",
    "        for j in train:\n",
    "            temp[j]=class_wise[i][j]\n",
    "            train_data.append(class_wise[i][j])\n",
    "        class_wise1.append(temp)\n",
    "        temp={}\n",
    "        for j in test:\n",
    "            temp[j]=class_wise[i][j]\n",
    "            test_data.append(class_wise[i][j])\n",
    "        class_wise2.append(temp)\n",
    "        # sum the values with same keys \n",
    "    counter = collections.Counter() \n",
    "    for d in train_data:  \n",
    "        counter.update(d) \n",
    "\n",
    "    train_set = dict(counter) \n",
    "    counter = collections.Counter() \n",
    "    for d in test_data:  \n",
    "        counter.update(d) \n",
    "\n",
    "    test_set = dict(counter)\n",
    "#     print(len(test_set))\n",
    "    return class_wise1,class_wise2,train_set,test_set\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell-pc\\\\Desktop\\\\IR_5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we wil get the sub-folders\n",
    "os.chdir('C:\\\\Users\\\\Dell-pc\\\\Desktop\\\\IR_4\\\\20_newsgroups')\n",
    "directories=os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminate=['comp.graphics', 'sci.med', 'talk.politics.misc', 'rec.sport.hockey','sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "files=[]\n",
    "union_set=[]\n",
    "postings={}\n",
    "corpus={}\n",
    "corpus1={}\n",
    "vocub={}\n",
    "vocub_main=[]\n",
    "term_frequency={}\n",
    "tf_main={}\n",
    "counter=0\n",
    "for i in range(len(directories)):\n",
    "    files.append(copy.deepcopy([]))\n",
    "\n",
    "for i in range(len(directories)):\n",
    "    \n",
    "    if directories[i] in eliminate:\n",
    "        print(counter)\n",
    "        counter=counter+1\n",
    "        address=os.getcwd()+\"\\\\\"+str(directories[i])\n",
    "        os.chdir(address)\n",
    "        files_in_dir=os.listdir()\n",
    "        for j in range(len(files_in_dir)):\n",
    "\n",
    "            files[i].append(directories[i]+\"/\"+files_in_dir[j])\n",
    "            address1=address+\"\\\\\"+files_in_dir[j]\n",
    "            c=[]\n",
    "            union_set=union_set+[files[i][j]]\n",
    "            offset=0\n",
    "            #stackoverflow\n",
    "            f=codecs.open(address1,'r',encoding='utf_8',errors=\"ignore\")\n",
    "\n",
    "            c = f.readlines()\n",
    "\n",
    "            final=[]\n",
    "            in1=c.index('\\n')\n",
    "            for k in range(0,len(c)):\n",
    "\n",
    "                final.append(c[k])\n",
    "            #Final is a list of sentences \n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            #Tokenizer previously used\n",
    "    #         tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "            temp1=[]\n",
    "            vocub={}\n",
    "            for k in range(len(final)):\n",
    "                temp=[]\n",
    "                temp=tokenizer.tokenize(final[k])\n",
    "                temp=[lemmatizer.lemmatize(w,pos='v') for w in temp]\n",
    "                temp =[w.lower() for w in temp]\n",
    "                \n",
    "             \n",
    "    #             print(\"Stop Wro\",temp)\n",
    "\n",
    "                for n in range(len(temp)):\n",
    "#                     if hasNumbers(temp[n]):\n",
    "#                         continue\n",
    "#                     if temp[n].isdigit():\n",
    "#                         continue\n",
    "                    temp2=copy.deepcopy(temp[n])\n",
    "                    t1=str(dash_improve(str(temp2)))\n",
    "\n",
    "                    temp2=t1\n",
    "                    t2=str(digit_improve(str(temp2)))\n",
    "\n",
    "                    temp2=t2\n",
    "                    t3=str(punc_improve(str(temp2)))\n",
    "\n",
    "                    temp2=t3\n",
    "\n",
    "                    if temp2==list(['']) or temp2==list([' ']) or temp2=='' or temp2==' ':\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp4=[]\n",
    "                        temp4.append(temp2)\n",
    "                        temp1=temp1+temp4\n",
    "                    if temp2 in vocub.keys():\n",
    "                        vocub[temp2]=vocub[temp2]+1\n",
    "                    else:\n",
    "                        vocub[temp2]=1\n",
    "\n",
    "            corpus1[files[i][j]]=temp1\n",
    "            corpus[files[i][j]]=vocub\n",
    "            \n",
    "            vocub_main=set(vocub_main).union(set(temp1))\n",
    "    #         if (counter % 5000)==0:\n",
    "    #             print(\"Count \",counter)\n",
    "    #         counter=counter+1\n",
    "            for k in set(corpus[files[i][j]]):\n",
    "                if k in postings.keys():\n",
    "                    if files[i][j] in postings[k]:\n",
    "                        pass\n",
    "                    else:\n",
    "\n",
    "                        postings[k].append(files[i][j])\n",
    "                    term_frequency[k]=term_frequency[k]+1\n",
    "                else:\n",
    "                    s=[]\n",
    "                    s.append(files[i][j])\n",
    "                    postings[k]=s\n",
    "                    term_frequency[k]=1  \n",
    "\n",
    "        os.chdir('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key which has no body : \n"
     ]
    }
   ],
   "source": [
    "print(\"Key which has no body : \")\n",
    "temp=[]\n",
    "for i in corpus.keys():\n",
    "    if corpus1[i]==[]:\n",
    "        print(\"Files are : \",i)\n",
    "        temp.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete the documents which has no body\n",
    "for i in temp:\n",
    "    corpus.pop(i, None)\n",
    "    corpus1.pop(i,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocub_main=list(vocub_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  0\n",
      "Counter :  2000000\n",
      "Counter :  4000000\n",
      "Counter :  6000000\n",
      "Counter :  8000000\n",
      "Counter :  10000000\n",
      "Counter :  12000000\n",
      "Counter :  14000000\n",
      "Counter :  16000000\n",
      "Counter :  18000000\n",
      "Counter :  20000000\n",
      "Counter :  22000000\n",
      "Counter :  24000000\n",
      "Counter :  26000000\n",
      "Counter :  28000000\n",
      "Counter :  30000000\n",
      "Counter :  32000000\n",
      "Counter :  34000000\n",
      "Counter :  36000000\n",
      "Counter :  38000000\n",
      "Counter :  40000000\n",
      "Counter :  42000000\n",
      "Counter :  44000000\n",
      "Counter :  46000000\n",
      "Counter :  48000000\n",
      "Counter :  50000000\n",
      "Counter :  52000000\n",
      "Counter :  54000000\n",
      "Counter :  56000000\n",
      "Counter :  58000000\n",
      "Counter :  60000000\n",
      "Counter :  62000000\n",
      "Counter :  64000000\n",
      "Counter :  66000000\n",
      "Counter :  68000000\n",
      "Counter :  70000000\n",
      "Counter :  72000000\n",
      "Counter :  74000000\n",
      "Counter :  76000000\n",
      "Counter :  78000000\n",
      "Counter :  80000000\n",
      "Counter :  82000000\n",
      "Counter :  84000000\n",
      "Counter :  86000000\n",
      "Counter :  88000000\n",
      "Counter :  90000000\n",
      "Counter :  92000000\n",
      "Counter :  94000000\n",
      "Counter :  96000000\n",
      "Counter :  98000000\n",
      "Counter :  100000000\n",
      "Counter :  102000000\n",
      "Counter :  104000000\n",
      "Counter :  106000000\n",
      "Counter :  108000000\n",
      "Counter :  110000000\n",
      "Counter :  112000000\n",
      "Counter :  114000000\n",
      "Counter :  116000000\n",
      "Counter :  118000000\n",
      "Counter :  120000000\n",
      "Counter :  122000000\n",
      "Counter :  124000000\n",
      "Counter :  126000000\n",
      "Counter :  128000000\n",
      "Counter :  130000000\n",
      "Counter :  132000000\n",
      "Counter :  134000000\n",
      "Counter :  136000000\n",
      "Counter :  138000000\n",
      "Counter :  140000000\n",
      "Counter :  142000000\n",
      "Counter :  144000000\n",
      "Counter :  146000000\n",
      "Counter :  148000000\n",
      "Counter :  150000000\n",
      "Counter :  152000000\n",
      "Counter :  154000000\n",
      "Counter :  156000000\n",
      "Counter :  158000000\n",
      "Counter :  160000000\n",
      "Counter :  162000000\n",
      "Counter :  164000000\n",
      "Counter :  166000000\n",
      "Counter :  168000000\n",
      "Counter :  170000000\n",
      "Counter :  172000000\n",
      "Counter :  174000000\n",
      "Counter :  176000000\n",
      "Counter :  178000000\n",
      "Counter :  180000000\n",
      "Counter :  182000000\n",
      "Counter :  184000000\n",
      "Counter :  186000000\n",
      "Counter :  188000000\n",
      "Counter :  190000000\n",
      "Counter :  192000000\n",
      "Counter :  194000000\n",
      "Counter :  196000000\n",
      "Counter :  198000000\n",
      "Counter :  200000000\n",
      "Counter :  202000000\n",
      "Counter :  204000000\n",
      "Counter :  206000000\n",
      "Counter :  208000000\n",
      "Counter :  210000000\n",
      "Counter :  212000000\n",
      "Counter :  214000000\n",
      "Counter :  216000000\n",
      "Counter :  218000000\n",
      "Counter :  220000000\n",
      "Counter :  222000000\n",
      "Counter :  224000000\n",
      "Counter :  226000000\n",
      "Counter :  228000000\n",
      "Counter :  230000000\n",
      "Counter :  232000000\n",
      "Counter :  234000000\n",
      "Counter :  236000000\n",
      "Counter :  238000000\n",
      "Counter :  240000000\n",
      "Counter :  242000000\n",
      "Counter :  244000000\n",
      "Counter :  246000000\n",
      "Counter :  248000000\n",
      "Counter :  250000000\n",
      "Counter :  252000000\n",
      "Counter :  254000000\n",
      "Counter :  256000000\n",
      "Counter :  258000000\n",
      "Counter :  260000000\n",
      "Counter :  262000000\n",
      "Counter :  264000000\n",
      "Counter :  266000000\n",
      "Counter :  268000000\n",
      "Counter :  270000000\n",
      "Counter :  272000000\n",
      "Counter :  274000000\n",
      "Counter :  276000000\n",
      "Counter :  278000000\n",
      "Counter :  280000000\n",
      "Counter :  282000000\n",
      "Counter :  284000000\n",
      "Counter :  286000000\n",
      "Counter :  288000000\n",
      "Counter :  290000000\n",
      "Counter :  292000000\n",
      "Counter :  294000000\n",
      "Counter :  296000000\n",
      "Counter :  298000000\n",
      "Counter :  300000000\n",
      "Counter :  302000000\n",
      "Counter :  304000000\n",
      "Counter :  306000000\n",
      "Counter :  308000000\n",
      "Counter :  310000000\n",
      "Counter :  312000000\n",
      "Counter :  314000000\n",
      "Counter :  316000000\n",
      "Counter :  318000000\n",
      "Counter :  320000000\n",
      "Counter :  322000000\n"
     ]
    }
   ],
   "source": [
    "#For idf calculations\n",
    "idf_vector={}\n",
    "idf=[]\n",
    "tf_idf_main={}\n",
    "counter=0\n",
    "for j in vocub_main:\n",
    "\n",
    "    count=0\n",
    "    for i in corpus.keys():\n",
    "        if counter%2000000==0:\n",
    "            print(\"Counter : \",counter)\n",
    "        counter+=1\n",
    "        if j in corpus[i]:\n",
    "            count+=1\n",
    "    idf_vector[j]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "tf_idf={}\n",
    "counter=0\n",
    "for j in range(len(vocub_main)):\n",
    "    \n",
    "    if counter%10000==0:\n",
    "        print(counter)\n",
    "    counter+=1\n",
    "    a=[]\n",
    "    c=0\n",
    "    for i in corpus.keys():\n",
    "        try:\n",
    "            if c<corpus[i][vocub_main[j]]/float(length(corpus[i])):\n",
    "                c=corpus[i][vocub_main[j]]/float(length(corpus[i]))\n",
    "        except:\n",
    "            pass\n",
    "    tf=c\n",
    "    tf_idf[vocub_main[j]]=tf*np.log(len(corpus)/(1+idf_vector[vocub_main[j]]))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocub_main=feature_selection(tf_idf,6000,len(vocub_main))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words for Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=[]\n",
    "count1=0\n",
    "count=0\n",
    "for i in corpus.keys():\n",
    "    temp=[]\n",
    "    if count1%1000==0:\n",
    "        count+=1\n",
    "    for j in vocub_main:\n",
    "        try:\n",
    "            temp.append(corpus[i][j])\n",
    "        except:\n",
    "            temp.append(0)\n",
    "            pass\n",
    "   \n",
    "    l2=np.linalg.norm(temp)\n",
    "    temp=[i/float(l2) for i in temp]\n",
    "    temp.append(count)\n",
    "    count1+=1\n",
    "    bow.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  0\n",
      "Counter :  1\n",
      "Counter :  2\n",
      "Counter :  3\n",
      "Counter :  4\n",
      "Counter :  5\n",
      "Counter :  6\n",
      "Counter :  7\n",
      "Counter :  8\n",
      "Counter :  9\n",
      "Counter :  10\n",
      "Counter :  11\n",
      "Counter :  12\n",
      "Counter :  13\n",
      "Counter :  14\n",
      "Counter :  15\n",
      "Counter :  16\n",
      "Counter :  17\n",
      "Counter :  18\n",
      "Counter :  19\n",
      "Counter :  20\n",
      "Counter :  21\n",
      "Counter :  22\n",
      "Counter :  23\n",
      "Counter :  24\n",
      "Counter :  25\n",
      "Counter :  26\n",
      "Counter :  27\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "after_cluster,rss1,num=k_means(bow,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS Value :  4739.591862463488\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS Value : \",rss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and True Labels for clusters\n",
    "true=[]\n",
    "predict=[]\n",
    "for i in range(len(after_cluster)):\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        true.append(after_cluster[i][j][-1])\n",
    "        predict.append(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clusters making \n",
    "after_cluster1=[]\n",
    "cluster=[]\n",
    "end=len(after_cluster[0][0])-1\n",
    "for i in range(len(after_cluster)):\n",
    "    temp=[]\n",
    "    temp1=[]\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        temp.append(after_cluster[i][j][:end])\n",
    "        temp1.append(after_cluster[i][j][-1])\n",
    "    after_cluster1+=temp\n",
    "    cluster.append(temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhoutte Coefficient :  0.025277310259814886\n"
     ]
    }
   ],
   "source": [
    "#Sihoutte Coefficient\n",
    "sc=silhouette_score(after_cluster1, predict, metric='euclidean')\n",
    "sample_silhouette_values = silhouette_samples(after_cluster1, predict)\n",
    "print(\"Silhoutte Coefficient : \",np.mean(sample_silhouette_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI :  0.1966845612334711\n"
     ]
    }
   ],
   "source": [
    "#ARI\n",
    "ari=adjusted_rand_score(true,predict)\n",
    "print(\"ARI : \",ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information :  0.2891753178676821\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Mutual Information : \",normalized_mutual_info_score(true, predict, average_method='warn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity :  0.512\n"
     ]
    }
   ],
   "source": [
    "print(\"Purity : \",purity(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words for Normalized Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1b94a27332b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0ml2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow=[]\n",
    "count1=0\n",
    "count=0\n",
    "for i in corpus.keys():\n",
    "    temp=[]\n",
    "    if count1%1000==0:\n",
    "        count+=1\n",
    "    for j in vocub_main:\n",
    "        try:\n",
    "            temp.append(corpus[i][j]/length(corpus[i]))\n",
    "        except:\n",
    "            temp.append(0)\n",
    "            pass\n",
    "    \n",
    "    l2=np.linalg.norm(temp)\n",
    "    temp=[i/float(l2) for i in temp]\n",
    "    temp.append(count)\n",
    "    count1+=1\n",
    "    bow.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "after_cluster,rss1=k_means_word2vec(bow,k,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n",
      "1270\n",
      "499\n",
      "940\n",
      "988\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(after_cluster)):\n",
    "    print(len(after_cluster[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS Value for clusters :  4790.37107860405\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS Value for clusters : \", rss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "true=[]\n",
    "predict=[]\n",
    "for i in range(len(after_cluster)):\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        true.append(after_cluster[i][j][-1])\n",
    "        predict.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clusters making \n",
    "after_cluster1=[]\n",
    "cluster=[]\n",
    "end=len(after_cluster[0][0])-1\n",
    "for i in range(len(after_cluster)):\n",
    "    temp=[]\n",
    "    temp1=[]\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        temp.append(after_cluster[i][j][:end])\n",
    "        temp1.append(after_cluster[i][j][-1])\n",
    "    after_cluster1+=temp\n",
    "    cluster.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity :  0.5214\n"
     ]
    }
   ],
   "source": [
    "print(\"Purity : \",purity(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sihoutte Coefficient\n",
    "sc=silhouette_score(after_cluster1, predict, metric='euclidean')\n",
    "sample_silhouette_values = silhouette_samples(after_cluster1, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhoutte Coefficient :  0.020036440362882744\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhoutte Coefficient : \",np.mean(sample_silhouette_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI :  0.26740654612268683\n"
     ]
    }
   ],
   "source": [
    "ari=adjusted_rand_score(true,predict)\n",
    "print(\"ARI : \",ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information :  0.3080916027343428\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Mutual Information : \",normalized_mutual_info_score(true, predict, average_method='warn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell-pc'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('IR_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'IR_5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-225c56a00225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IR_5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'IR_5'"
     ]
    }
   ],
   "source": [
    "os.chdir('IR_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=[]\n",
    "count=-1\n",
    "count1=-1\n",
    "for i in corpus.keys():\n",
    "    \n",
    "    temp=[]\n",
    "    count+=1\n",
    "    for j in corpus[i]:\n",
    "        try:\n",
    "            tf=corpus[i][j]/float(length(corpus[i]))\n",
    "            \n",
    "            \n",
    "            vector=model[j].tolist()\n",
    "            new_vec=[i*tf for i in vector]\n",
    "            temp.append(new_vec)\n",
    "        except:\n",
    "            pass\n",
    "#     print(temp)\n",
    "    temp1=np.mean(temp,axis=0)\n",
    "    temp1=temp1.tolist()\n",
    "    if count%1000==0:\n",
    "        count1+=1\n",
    "#     l2=np.linalg.norm(temp1)\n",
    "#     temp1=[i/float(l2) for i in temp1]\n",
    "    temp1.append(count1)\n",
    "    bow.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell-pc\\\\Desktop\\\\IR_5'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping into label_ table\n",
    "import pickle\n",
    "with open(\"pickle_bow_word2vec.txt\", 'wb') as f:\n",
    "    pickle.dump(bow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickled data\n",
    "bow = pickle.load(open(\"temp.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  0\n",
      "Counter :  1\n",
      "Counter :  2\n",
      "Counter :  3\n",
      "Counter :  4\n",
      "Counter :  5\n",
      "Counter :  6\n",
      "Counter :  7\n",
      "Counter :  8\n",
      "Counter :  9\n",
      "Counter :  10\n",
      "Counter :  11\n",
      "Counter :  12\n",
      "Counter :  13\n",
      "Counter :  14\n",
      "Counter :  15\n",
      "Counter :  16\n",
      "Counter :  17\n",
      "Counter :  18\n",
      "Counter :  19\n",
      "Counter :  20\n",
      "Counter :  21\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "after_cluster,rss1=k_means_word2vec(bow,k,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "521\n",
      "1150\n",
      "1771\n",
      "1159\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(after_cluster)):\n",
    "    print(len(after_cluster[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS Value for clusters :  83.87050931035948\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS Value for clusters : \", rss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "true=[]\n",
    "predict=[]\n",
    "for i in range(len(after_cluster)):\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        true.append(after_cluster[i][j][-1])\n",
    "        predict.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clusters making \n",
    "after_cluster1=[]\n",
    "cluster=[]\n",
    "end=len(after_cluster[0][0])-1\n",
    "for i in range(len(after_cluster)):\n",
    "    temp=[]\n",
    "    temp1=[]\n",
    "    for j in range(len(after_cluster[i])):\n",
    "        temp.append(after_cluster[i][j][:end])\n",
    "        temp1.append(after_cluster[i][j][-1])\n",
    "    after_cluster1+=temp\n",
    "    cluster.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity :  0.4102\n"
     ]
    }
   ],
   "source": [
    "print(\"Purity : \",purity(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sihoutte Coefficient\n",
    "sc=silhouette_score(after_cluster1, predict, metric='euclidean')\n",
    "sample_silhouette_values = silhouette_samples(after_cluster1, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI :  0.1345113914289292\n"
     ]
    }
   ],
   "source": [
    "ari=adjusted_rand_score(true,predict)\n",
    "print(\"ARI : \",ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhoutte Coefficient :  0.04708322638466295\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhoutte Coefficient : \",np.mean(sample_silhouette_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information :  0.20113254801202454\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Mutual Information : \",normalized_mutual_info_score(true, predict, average_method='warn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
